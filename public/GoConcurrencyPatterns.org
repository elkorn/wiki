#+FILETAGS: :vimwiki:

* GoConcurrencyPatterns
** Go concurrency patterns
# %toc

(from http://blog.golang.org/pipelines )

Go's concurrency primitives lead to constructing streaming data pipelines.

A pipeline is a series of *stages* cnnected by channels, where each stage is a group of
goroutines running the same function.

In each stage, the goroutines:
*** receive values from upstream via inbound channels,
*** perform some function on that data,
*** send values downstream via outbound channels.

(There is a strong analogy with node.js streams here.)

Each stage has any number of channels, except the first and the last one - which have
only inbound and outbound, respectively.

The first stage is called the _source_ or _producer_, the last stage- a _sink_ or _consumer_.
(Parallel to general concurrent programming terms here)

*** Example: squaring numbers
_Consumer_ stage: `gen`

#+begin_example
func gen(nums ...int) <-chan int {
    out := make(chan int)
    go func() {
        for _, n := range nums {
            out <- n
        }
        close(out)
    }()
    return out
}
#+end_example

_Worker_ stage: `sq`

#+begin_example
func sq(in <-chan int) <-chan int {
    out := make(chan int)
    go func() {
        for n := range in {
            out <- n * n
        }
        close(out)
    }()
    return out
}
#+end_example

_Producer_ stage: `main`
#+begin_example
func main() {
    // Set up the pipeline.
    c := gen(2, 3)
    out := sq(c)

    // Consume the output.
    fmt.Println(<-out) // 4
    fmt.Println(<-out) // 9
}
#+end_example

Note that the input and output channel of `sq` have the same type.
This makes it composable any number of times.

*** Fan-out, fan-in

*Fan-out* means that multiple functions may read from the same channel until it's
closed.
Due to this, work can be distributed amongst workers to parallelize CPU use and I/O.

*Fan-in* means multiplexing multiple input channels onto a single output channel which
is closed after all inputs are closed.

We can translate the `sq` pipeline to run two instances.
#+begin_example
func main() {
    in := gen(2, 3)

    // Distribute the sq work across two goroutines that both read from in.
    c1 := sq(in)
    c2 := sq(in)

    // Consume the merged output from c1 and c2.
    for n := range merge(c1, c2) {
        fmt.Println(n) // 4 then 9, or 9 then 4
    }
}
#+end_example

`merge` converts multiple channels into one, by starting a goroutine for each one and
copying their values to the output.
After all inputs are closed, an additional goroutine is started to close the outbound
channel after everything is sent.
#+begin_example
func merge(cs ...<-chan int) <-chan int {
    var wg sync.WaitGroup
    out := make(chan int)

    // Start an output goroutine for each input channel in cs.  output
    // copies values from c to out until c is closed, then calls wg.Done.
    output := func(c <-chan int) {
        for n := range c {
            out <- n
        }
        wg.Done()
    }
    wg.Add(len(cs))
    for _, c := range cs {
        go output(c)
    }

    // Start a goroutine to close out once all the output goroutines are
    // done.  This must start after the wg.Add call.
    go func() {
        wg.Wait()
        close(out)
    }()
    return out
}
#+end_example

`sync.WaitGroup` acts as a semaphore.
Sending anything to a closed channel causes a panic, so the barrier takes care of that.

*** Stopping short

Having given the following:
**** stages close outbound channels when all send operations are done,
**** stages keep receiving from inbound channels until they're closed,

one can write a channel interaction as `range`.

In reality though, stages do not always act like that.
Sometimes the receiver may only need a subset of values to proceed.
More often, there is an error in one of the early stages that causes it to exit early.

The receiver should not have to wait for _all_ the values to arrive and we should be
able to make the earlier stages stop producing values that later stages do not need.

In the pipeline created earlier, blocked goroutines will stay around forever, causing a
resource leak.

One way to avoid such a situation is to change the outbound channels to their buffered
counterparts.
This alows for some simplifications:
#+begin_example
func gen(nums ...int) <-chan int {
    out := make(chan int, len(nums))
    for _, n := range nums {
        out <- n
    }
    close(out)
    return out
}

func merge(cs ...<-chan int) <-chan int {
    var wg sync.WaitGroup
    out := make(chan int, 1) // enough space for the unread inputs
    // ... the rest is unchanged ...
#+end_example

This is bad code though - the buffer size choice depends on knowing the number of
values `merge` will receive and the downstream stages will consume.
It's obviously fragile.

A better alternative is to have cancellation channels as a way of downstream stages
signalling to the upstream that they no longer need data.

*** Explicit cancellation

The last stage can signal previous ones that it no longer needs anything by sending
a signal on a `done` channel.

In this example, it sends two values, since there are possibly two blocked senders.
#+begin_example
func main() {
    in := gen(2,3)

    // Distribute the sq work across two goroutines reading from in.
    c1 := sq(in)
    c2 := sq(in)
    
    // Consume the first value from output.
    done := make(chan struct{}, 2)
    out := merge(done, c1, c2)
    fmt.Println(<-out) // 4 or 9

    // Tell the remaining senders we're leaving.
    done <- struct{}{}
    done <- struct{}{}
}
#+end_example

Now, the senders must use `select` to take the cancellation channel into account.
The value of `done` is an empty struct, because the type does not matter here - 
anything can be used.

I prefer using `bool` and creating a `type Signal chan bool` for such usage.
#+begin_example
func merge9done <- chan struct{}, cs ...<-chan int) <-chan int {
    var wg sync.WaitGroup
    out := make(chan int)
    
    // Start an output goroutine for each input channel in cs.
    // Output copies values from c to out until c is closed or it receives a value
    // from done, then calls wg.Done.

    output := func(c <-chan int) {
        for n := range c {
            select {
                case out <- n:
                case <- done:
                    // drain the input channel to exit.
            }
        }
        
        wg.Done()
    }

    // The rest remains unchanged.
}
#+end_example

The problem here is that each downstream receiver needs to possess the knowledge of
the number of potentially blocked upstream senders and signal them all on return.

By closing a channel, though, we tell an unknown, unbounded number of goroutines to
stop sending their values downstream. This is thanks to the fact that in Go, a receive
operation on a closed channel can always proceed immediately, yielding the element 
type's null value.

What needs to be done then is to extend each of the pipeline functions to accept
`done` as a paremeter and arrange the close to happen via `defer` (remember about
performance overhead).
This way, all return patterns from downstream will signal the pipeline stages to exit.

#+begin_example
func main() {
    done := make(chan struct{})
    defer close(done)

    in := gen(2,3)
    c1 := sq(done, in)
    c2 := sq(done, in)

    out := merge(done, c1, c2)
    fmt.Println(<-out)

    // done will be closed by the deferred call.
}
#+end_example

This allows the `output` routine in `merge` to stop draining its inbound channel,
since it's certain that `sq` ill stop attempting to send when `done` is closed.
Only thing we ensure here is that `wg.Done` is called on all return paths.
#+begin_example
func merge(done <- chan struct{}, cs ...<-chan int) <-chan int {
    var wg sync.WaitGroup
    out := make(chan int)
    
    output := func(c <-chan int) {
        defer wg.Done()
        for n := range c {
            select {
                case out <- n:
                case <- done:
                    return
            }
        }
    }

    // The rest remains the same.
}
#+end_example

Similar pattern is applied to `sq`:
#+begin_example
func sq(done <-chan struct{}, in <-chan int) <-chan int {
    out := make(chan int)
    go func() {
        defer close(out)
        for n := range in {
            select {
                case out <- n * n:
                case <- done:
                    return
            }
        }
    }()

    return out
}
#+end_example

General guidelines for pipeline construction:
**** Stages close their outbound channels hen all the send operations are done.
**** Stages keep receiving values from inbound channels until those are closed or senders are unblocked.

Senders are unblocked by ensuring there is enough buffer for all sent values or by
signalling them the receiver is abandoning the channel.

*** Example: digesting a tree

The example will perform `md5sum` for each regular file in a directory, sorted by
filename.

The helper function, `MD5All` returns a map from path name to digest value.
The main function sorts and prints the results.
#+begin_example
func main() {
    m, err := MD5All(os.Args[1])
    if nil != err {
        fmt.Println(err)
        return
    }

    var paths []string
    for path := range m {
        paths = append(paths, path)
    }

    sort.Strings(paths)     // built-in
    for _, path := range paths {
        fmt.Printf("%x %s
", m[path], path)
    }
}
#+end_example

In a serial implementation, `MD5All` simply reads and sums each file as it walks the
tree.

#+begin_example
func MD5All(root string) (map[string][md5.Size]byte, error) {
    m := make(map[string][md5.Size]byte)
    err := filepath.Walk(root, func(path string, info os.FileInfo, err error) error {
        if nil != err {
            return err
        }

        if !info.Mode().IsRegular() {
            return nil
        }

        data, err := ioutil.ReadFile(path)
        if nil != err {
            return err
        }

        m[path] = md5.Sum(data)
        return nil
    })

    if nil != err {
        return nil, err
    }

    return m, nil
#+end_example

**** Parallel digestion

To parallelize `MD5All`, it needs to be split into a 2-stage pipeline.
First stage is `sumFiles`, which walks the tree, digests each file in a separate
goroutine and sends it to a channel with value type `result`.
#+begin_example
type result struct {
    path string
    sum [md5.Size]byte
    err error
}
#+end_example

It resturns 2 channels:
***** one for `result`s
***** one for error returned by `filepath.Walk`.

The `walk` function starts a new goroutine to process each file and then checks `done`.
If it's closed, the walk stops immediately.
#+begin_example
func sumFiles(done <-chan struct{}, root string) (<-chan result, <-chan error) {
    c := make(chan result)
    errc := make(chan error, 1)
    
    go func() {
        var wg sync.WaitGroup
        err := filepath.Walk(root, func(path string, info os.FileInfo, err error) error {
            if nil != err {
                return err
            }

            if !info.Mode().IsRegular() {
                return nil
            }

            wg.Add(1)   // raise the semaphore
            go func() {
                data, err := ioutil.Readfile(path)
                select {
                    case c <-result{path, md5, Sum(data), err}:
                    case <-done:
                }

                wg.Done()
            }()

            select {
                case <-done:
                    return errors.New("walk canceled")
                default:
                    return nil
            }
        })
        
        // All calls to wg.Add are done here.
        // Close c when all sends are done.
        go func() {
            wg.Wait()
            close(c)
        }()

        errc <- err // errc is buffered so no select needed.
    }()

    return c, errc
}
#+end_example

`MD5All` receives the digest values from `c` and returns early on error, closing `done`.
#+begin_example
func MD5All(root string) (map[string][md5.Size]byte. error) {
    done := make(chan struct{})
    defer close(done)
    c, errc := sumFiles(done, root)
    m := make(map[string][md5.Size]byte)
    for r := range c {
        if nil != r.err {
            return nil, r.err
        }

        m[r.path] = r.sum
    }

    if err := <-errc; nil != err {
        return nil, err
    }

    return m, nil
}
#+end_example

See that nowhere in `MD5All` anything is being sent to `done`.
Closing the channel is a message in itself.

*** Bounded parallelism
The current `MD5All` implementation starts a new goroutine for each file.
That may cause high memory usage, exceeding available resources.

To limit these allocations, we can bound the number of files being processed in
parallel.
It can by done by creating a fixed number of goroutines for reading files.
The pipeline would then consist of three stages:
**** walk the tree
**** read and digest the files
**** collect the digests

(This looks an awful lot like map/reduce.)

The first stage emits the paths of regular files in the tree:
#+begin_example
func walkFiles(done <-chan struct{}, root string) (<-chan string, <-chan error) {
    paths := make(chan string)
    errc := make(chan error, 1)
    go func () {
        defer close(paths)
        errc <- filepath.Walk(root, func(path string, info os.FileInfo, err error) error {
            if nil != err {
                return err
            }

            if !info.Mode().IsRegular() {
                return nil
            }

            select {
                case paths <-path:
                case <-done:
                    return errors.New("walk canceled")
            }

            return nil
        })
    }()

    return paths, errc
}
#+end_example

The nest stage starts a fixed number of `digester` goroutines, looking as follows.
#+begin_example
func digester(done <-chan struct{}, paths <-chan string, c chan<- result) {
    for path := range paths {
        data, err := ioutil.ReadFile(path)
        select {
            case c <- result{path, md5.Sum(data), err}:
            case <-done:
                return
        }
    }
}
#+end_example

It is important for `digester` not to close its output channel, as multiple goroutines
are sending on it.
It's the job of `MD5All` to arrange for `c` to be closed when all `digester`s are done.
#+begin_example
c := make(chan result)
var wg sync.WaitGroup
const numDigesters = 20
wg.Add(numDigesters)
for i := 0; i < numDigesters; i++ {
    go func() {
        digester(done, paths, c)
        wg.Done()
    }()
}
go func() {
    wg.Wait()
    close(c)
}()
#+end_example

The alternative here would be for each `digester` to create and return its own output
channel - this would require fanning them in within `MD5All`.

The final stage receives all the `result`s and then checks `errc`.
It's important nto to check `errc` any sooner, because `walkFiles` may have not
finished yet.
#+begin_example
m := make(map[string][md5.Size]byte)
for r := range c {
    if nil != r.err {
        return nil, r.err
    }

    m[r.path] = r.sum
}

if err := <-errc: nil != err {
    return nil, err
}

return m, err
#+end_example

My observation is that there is not much sense to returning `nil` in case of an error,
as the failure may not be total.
If we were to return `nil` on any error occurence, it would make sense to break all
computations in that moment.
