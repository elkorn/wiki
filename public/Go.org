#+FILETAGS: :vimwiki:

* Go
** Go

[[GoArchitecture]]

[[GoConcurrencyPatterns]]

[[Libraries]]

[[Profiling]]

[[GoPractices]]

*** GoArchitecture
**** Go - Architecture
# %toc

***** Structuring applications
****** Don't use global variables

The prime example is `net/http`'s `handleFunc`. The simplest way is to pass a function there:
#+begin_example
package main
import (
 ‚Äúfmt‚Äù
 ‚Äúnet/http‚Äù
)
func main() {
    http.HandleFunc(‚Äú/hello‚Äù, hello)
    http.ListenAndServe(‚Äú:8080", nil)
}
func hello(w http.ResponseWriter, r *http.Request) {
    fmt.Fprintf(w, ‚Äúhi!‚Äù)
}
#+end_example

Unfortunately, ay state has to be accessed through a global variable then.

A better alternative is to use an encompassing type:
#+begin_example
type HelloHandler struct {
    db *sql.DB
}
func (h *HelloHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) {
    var name string
    // Execute the query.
    row := h.db.QueryRow(‚ÄúSELECT myname FROM mytable‚Äù)
    if err := row.Scan(&name); err != nil {
        http.Error(w, err.Error(), 500)
        return
    }
    // Write it back to the client.
    fmt.Fprintf(w, ‚Äúhi %s!
‚Äù, name)
}

func main() {
    // Open our database connection.
    db, err := sql.Open(‚Äúpostgres‚Äù, ‚Äú‚Ä¶‚Äù)
    if err != nil {
        log.Fatal(err)
    }
    // Register our handler.
    http.Handle(‚Äú/hello‚Äù, &HelloHandler{db: db})
    http.ListenAndServe(‚Äú:8080", nil)
}
#+end_example

See that the state has been nicely encapsulated.
Also, the handler becomes unit-testable.

****** Separate your binary from your application

Placing the `main.go` file along with the binary causes the appliaction to be unusable as a library and constrains it to have only one binary.

A more flexible approach is to use an `src` (or whatever) directory, whose each subdir is effectively a binary:
#+begin_example
camlistore/
  cmd/
    camget/
      main.go
    cammount/
      main.go
    camput/
      main.go
    camtool/
      main.go
#+end_example

Additional benefit of this approach is that you get a cleaner abstraction of what code belongs to the core domain logic and what is just a client.
Remember that a binary is a client for the library you are creating, an interface for the user to interact with it.
An example of an `adder` package, sproting a CLI and a Web interface:
#+begin_example
adder/
  adder.go          // core
  cmd/
    adder/
      main.go       // CLI
    adder-server/
      main.go       // Web
#+end_example

To install the whole bundle, use `go get` with an ellipsis: `go get github.com/elkorn/adder/...`.

****** Wrap types for application-specific context

This is a universal rule - create proxies that access underlying generic components only through specific, application-bound context.
A good example is the database:
#+begin_example
package myapp
import (
    "database/sql"
)
type DB struct {
    *sql.DB
}
type Tx struct {
    *sql.Tx
}

// Open returns a DB reference for a data source.
func Open(dataSourceName string) (*DB, error) {
    db, err := sql.Open("postgres", dataSourceName)
    if err != nil {
        return nil, err
    }
    return &DB{db}, nil
}
// Begin starts an returns a new transaction.
func (db *DB) Begin() (*Tx, error) {
    tx, err := db.DB.Begin()
    if err != nil {
        return nil, err
    }
    return &Tx{tx}, nil
}

// CreateUser creates a new user.
// Returns an error if user is invalid or the tx fails.
func (tx *Tx) CreateUser(u *User) error {
    // Validate the input.
    if u == nil {
        return errors.New("user required")
    } else if u.Name == "" {
        return errors.New("name required")
    }

    // Perform the actual insert and return any errors.
    return tx.Exec(`INSERT INTO users (...) VALUES`, ...)
}
#+end_example

The context is isolated in a relevant component, which exposes a simple API that can accomodate any changes You might want to introduce later.

The `Tx` example provides an additional boon of transactional composition, e.g. for creating multiple users:
#+begin_example
tx, _ := db.Begin()
for _, u := range users {
    tx.CreateUser(u)
}
tx.Commit()
#+end_example

****** Don't go crazy with subpackages

The most important thing to note here is that the need for a packagae should be dictated by logical functionality, not the number of files.
Larger packages are OK.

A few points to take heed of:
******* *Group related types and code together in each file.*
******* *The most important type goes at the top.* Types of decreasing importance go lower.
******* *Once your app exceeds 10k loc*, reevaluate ts structure in the context of whether could it be broken into smaller projects.

***** Structuring tests

(from https://medium.com/@benbjohnson/structuring-tests-in-go-46ddee7a25c)

****** Don't use frameworks
Go's `testing` is good enough.
Author proposes the following helpers to make assertions less verbose:
#+begin_example
import (
    "fmt"
    "path/filepath"
    "runtime"
    "reflect"
    "testing"
)

// assert fails the test if the condition is false.
func assert(tb testing.TB, condition bool, msg string, v ...interface{}) {
    if !condition {
        _, file, line, _ := runtime.Caller(1)
        fmt.Printf("[31m%s:%d: "+msg+"[39m

", append([]interface{}{filepath.Base(file), line}, v...)...)
        tb.FailNow()
    }
}

// ok fails the test if an err is not nil.
func ok(tb testing.TB, err error) {
    if err != nil {
        _, file, line, _ := runtime.Caller(1)
        fmt.Printf("[31m%s:%d: unexpected error: %s[39m

", filepath.Base(file), line, err.Error())
        tb.FailNow()
    }
}

// equals fails the test if exp is not equal to act.
func equals(tb testing.TB, exp, act interface{}) {
    if !reflect.DeepEqual(exp, act) {
        _, file, line, _ := runtime.Caller(1)
        fmt.Printf("[31m%s:%d:

	exp: %#v

	got: %#v[39m

", filepath.Base(file), line, exp, act)
        tb.FailNow()
    }
}
#+end_example

Benefit:

#+begin_example
// Before
func TestSomething(t *testing.T) {
    value, err := DoSomething()
    if err != nil {
        t.Fatalf("DoSomething() failed: %s", err)
    }
    if value != 100 {
        t.Fatalf("expected 100, got: %d", value)
    }
}

// After
func TestSomething(t *testing.T) {
    value, err := DoSomething()
    ok(t, err)
    equals(t, 100, value)
}
#+end_example

****** Use the '..._test' package

E.g. for `package myapp`, keep the following files in its directory:
******* `myapp.go`
******* `myapp_test.go`

Then, specify `package myapp_test` in `myapp_test.go`.
Such setup will help maintain the proper visibility of things while testing.

Also, it's the only case where Go will allow multiple packages in one directory.

****** Use test-specific types

For example, set up a test database using a temp file and providing a `close` function for simple teardown.

#+begin_example
type TestDB struct {
    *DB // the original application-specific DB type
}
// NewTestDB returns a TestDB using a temporary path.
func NewTestDB() *TestDB {
    // Retrieve a temporary path.
    f, err := ioutil.TempFile("", "")
    if err != nil {
        panic("temp file: %s", err)
    }
    path := f.Name()
    f.Close()
    os.Remove(path)
    // Open the database.
    db, err := Open(path, 0600)
    if err != nil {
        panic("open: %s", err)
    }
    // Return wrapped type.
    return &TestDB{db}
}
// Close and delete Bolt database.
func (db *TestDB) Close() {
    defer os.Remove(db.Path())
    db.DB.Close()
}
#+end_example

****** Use inline interfaces and simple mocks

The idea is that the caller should create the interface it wants 
instead of the callee having to provide one.

An example is given based on a Yo app client.
#+begin_example
package yo
type Client struct {}
// Send sends a "yo" to someone.
func (c *Client) Send(recipient string) error
// Yos retrieves a list of my yo's.
func (c *Client) Yos() ([]*Yo, error)
#+end_example

To make it able to send Yo's, the following can be declared.
#+begin_example
package myapp
type MyApplication struct {
    YoClient interface {
        Send(string) error
    }
}
func (a *MyApplication) Yo(recipient string) {
    return a.YoClient.Send(recipient)
}
#+end_example

In `main.go`, a client can be injected.
#+begin_example
package main
func main() {
    c := yo.NewClient()
    a := myapp.MyApplication{}
    a.YoClient = c
    ...
}
#+end_example

As well as in tests.
#+begin_example
package myapp_test
// TestYoClient provides mockable implementation of yo.Client.
type TestYoClient struct {
    SendFunc func(string) error
}
func (c *TestYoClient) Send(recipient string) error {
    return c.SendFunc(recipient)
}
func TestMyApplication_SendYo(t *testing.T) {
    c := &TestYoClient{}
    a := &MyApplication{YoClient: c}
    // Mock our send function to capture the argument.
    var recipient string
    c.SendFunc = func(s string) error {
        recipient = s
        return nil
    }
    // Send the yo and verify the recipient.
    err := a.Yo("susy")
    ok(t, err)
    equals(t, "susy", recipient)
}
#+end_example
*** GoConcurrencyPatterns
**** Go concurrency patterns
# %toc

(from http://blog.golang.org/pipelines )

Go's concurrency primitives lead to constructing streaming data pipelines.

A pipeline is a series of *stages* cnnected by channels, where each stage is a group of
goroutines running the same function.

In each stage, the goroutines:
***** receive values from upstream via inbound channels,
***** perform some function on that data,
***** send values downstream via outbound channels.

(There is a strong analogy with node.js streams here.)

Each stage has any number of channels, except the first and the last one - which have
only inbound and outbound, respectively.

The first stage is called the _source_ or _producer_, the last stage- a _sink_ or _consumer_.
(Parallel to general concurrent programming terms here)

***** Example: squaring numbers
_Consumer_ stage: `gen`

#+begin_example
func gen(nums ...int) <-chan int {
    out := make(chan int)
    go func() {
        for _, n := range nums {
            out <- n
        }
        close(out)
    }()
    return out
}
#+end_example

_Worker_ stage: `sq`

#+begin_example
func sq(in <-chan int) <-chan int {
    out := make(chan int)
    go func() {
        for n := range in {
            out <- n * n
        }
        close(out)
    }()
    return out
}
#+end_example

_Producer_ stage: `main`
#+begin_example
func main() {
    // Set up the pipeline.
    c := gen(2, 3)
    out := sq(c)

    // Consume the output.
    fmt.Println(<-out) // 4
    fmt.Println(<-out) // 9
}
#+end_example

Note that the input and output channel of `sq` have the same type.
This makes it composable any number of times.

***** Fan-out, fan-in

*Fan-out* means that multiple functions may read from the same channel until it's
closed.
Due to this, work can be distributed amongst workers to parallelize CPU use and I/O.

*Fan-in* means multiplexing multiple input channels onto a single output channel which
is closed after all inputs are closed.

We can translate the `sq` pipeline to run two instances.
#+begin_example
func main() {
    in := gen(2, 3)

    // Distribute the sq work across two goroutines that both read from in.
    c1 := sq(in)
    c2 := sq(in)

    // Consume the merged output from c1 and c2.
    for n := range merge(c1, c2) {
        fmt.Println(n) // 4 then 9, or 9 then 4
    }
}
#+end_example

`merge` converts multiple channels into one, by starting a goroutine for each one and
copying their values to the output.
After all inputs are closed, an additional goroutine is started to close the outbound
channel after everything is sent.
#+begin_example
func merge(cs ...<-chan int) <-chan int {
    var wg sync.WaitGroup
    out := make(chan int)

    // Start an output goroutine for each input channel in cs.  output
    // copies values from c to out until c is closed, then calls wg.Done.
    output := func(c <-chan int) {
        for n := range c {
            out <- n
        }
        wg.Done()
    }
    wg.Add(len(cs))
    for _, c := range cs {
        go output(c)
    }

    // Start a goroutine to close out once all the output goroutines are
    // done.  This must start after the wg.Add call.
    go func() {
        wg.Wait()
        close(out)
    }()
    return out
}
#+end_example

`sync.WaitGroup` acts as a semaphore.
Sending anything to a closed channel causes a panic, so the barrier takes care of that.

***** Stopping short

Having given the following:
****** stages close outbound channels when all send operations are done,
****** stages keep receiving from inbound channels until they're closed,

one can write a channel interaction as `range`.

In reality though, stages do not always act like that.
Sometimes the receiver may only need a subset of values to proceed.
More often, there is an error in one of the early stages that causes it to exit early.

The receiver should not have to wait for _all_ the values to arrive and we should be
able to make the earlier stages stop producing values that later stages do not need.

In the pipeline created earlier, blocked goroutines will stay around forever, causing a
resource leak.

One way to avoid such a situation is to change the outbound channels to their buffered
counterparts.
This alows for some simplifications:
#+begin_example
func gen(nums ...int) <-chan int {
    out := make(chan int, len(nums))
    for _, n := range nums {
        out <- n
    }
    close(out)
    return out
}

func merge(cs ...<-chan int) <-chan int {
    var wg sync.WaitGroup
    out := make(chan int, 1) // enough space for the unread inputs
    // ... the rest is unchanged ...
#+end_example

This is bad code though - the buffer size choice depends on knowing the number of
values `merge` will receive and the downstream stages will consume.
It's obviously fragile.

A better alternative is to have cancellation channels as a way of downstream stages
signalling to the upstream that they no longer need data.

***** Explicit cancellation

The last stage can signal previous ones that it no longer needs anything by sending
a signal on a `done` channel.

In this example, it sends two values, since there are possibly two blocked senders.
#+begin_example
func main() {
    in := gen(2,3)

    // Distribute the sq work across two goroutines reading from in.
    c1 := sq(in)
    c2 := sq(in)
    
    // Consume the first value from output.
    done := make(chan struct{}, 2)
    out := merge(done, c1, c2)
    fmt.Println(<-out) // 4 or 9

    // Tell the remaining senders we're leaving.
    done <- struct{}{}
    done <- struct{}{}
}
#+end_example

Now, the senders must use `select` to take the cancellation channel into account.
The value of `done` is an empty struct, because the type does not matter here - 
anything can be used.

I prefer using `bool` and creating a `type Signal chan bool` for such usage.
#+begin_example
func merge9done <- chan struct{}, cs ...<-chan int) <-chan int {
    var wg sync.WaitGroup
    out := make(chan int)
    
    // Start an output goroutine for each input channel in cs.
    // Output copies values from c to out until c is closed or it receives a value
    // from done, then calls wg.Done.

    output := func(c <-chan int) {
        for n := range c {
            select {
                case out <- n:
                case <- done:
                    // drain the input channel to exit.
            }
        }
        
        wg.Done()
    }

    // The rest remains unchanged.
}
#+end_example

The problem here is that each downstream receiver needs to possess the knowledge of
the number of potentially blocked upstream senders and signal them all on return.

By closing a channel, though, we tell an unknown, unbounded number of goroutines to
stop sending their values downstream. This is thanks to the fact that in Go, a receive
operation on a closed channel can always proceed immediately, yielding the element 
type's null value.

What needs to be done then is to extend each of the pipeline functions to accept
`done` as a paremeter and arrange the close to happen via `defer` (remember about
performance overhead).
This way, all return patterns from downstream will signal the pipeline stages to exit.

#+begin_example
func main() {
    done := make(chan struct{})
    defer close(done)

    in := gen(2,3)
    c1 := sq(done, in)
    c2 := sq(done, in)

    out := merge(done, c1, c2)
    fmt.Println(<-out)

    // done will be closed by the deferred call.
}
#+end_example

This allows the `output` routine in `merge` to stop draining its inbound channel,
since it's certain that `sq` ill stop attempting to send when `done` is closed.
Only thing we ensure here is that `wg.Done` is called on all return paths.
#+begin_example
func merge(done <- chan struct{}, cs ...<-chan int) <-chan int {
    var wg sync.WaitGroup
    out := make(chan int)
    
    output := func(c <-chan int) {
        defer wg.Done()
        for n := range c {
            select {
                case out <- n:
                case <- done:
                    return
            }
        }
    }

    // The rest remains the same.
}
#+end_example

Similar pattern is applied to `sq`:
#+begin_example
func sq(done <-chan struct{}, in <-chan int) <-chan int {
    out := make(chan int)
    go func() {
        defer close(out)
        for n := range in {
            select {
                case out <- n * n:
                case <- done:
                    return
            }
        }
    }()

    return out
}
#+end_example

General guidelines for pipeline construction:
****** Stages close their outbound channels hen all the send operations are done.
****** Stages keep receiving values from inbound channels until those are closed or senders are unblocked.

Senders are unblocked by ensuring there is enough buffer for all sent values or by
signalling them the receiver is abandoning the channel.

***** Example: digesting a tree

The example will perform `md5sum` for each regular file in a directory, sorted by
filename.

The helper function, `MD5All` returns a map from path name to digest value.
The main function sorts and prints the results.
#+begin_example
func main() {
    m, err := MD5All(os.Args[1])
    if nil != err {
        fmt.Println(err)
        return
    }

    var paths []string
    for path := range m {
        paths = append(paths, path)
    }

    sort.Strings(paths)     // built-in
    for _, path := range paths {
        fmt.Printf("%x %s
", m[path], path)
    }
}
#+end_example

In a serial implementation, `MD5All` simply reads and sums each file as it walks the
tree.

#+begin_example
func MD5All(root string) (map[string][md5.Size]byte, error) {
    m := make(map[string][md5.Size]byte)
    err := filepath.Walk(root, func(path string, info os.FileInfo, err error) error {
        if nil != err {
            return err
        }

        if !info.Mode().IsRegular() {
            return nil
        }

        data, err := ioutil.ReadFile(path)
        if nil != err {
            return err
        }

        m[path] = md5.Sum(data)
        return nil
    })

    if nil != err {
        return nil, err
    }

    return m, nil
#+end_example

****** Parallel digestion

To parallelize `MD5All`, it needs to be split into a 2-stage pipeline.
First stage is `sumFiles`, which walks the tree, digests each file in a separate
goroutine and sends it to a channel with value type `result`.
#+begin_example
type result struct {
    path string
    sum [md5.Size]byte
    err error
}
#+end_example

It resturns 2 channels:
******* one for `result`s
******* one for error returned by `filepath.Walk`.

The `walk` function starts a new goroutine to process each file and then checks `done`.
If it's closed, the walk stops immediately.
#+begin_example
func sumFiles(done <-chan struct{}, root string) (<-chan result, <-chan error) {
    c := make(chan result)
    errc := make(chan error, 1)
    
    go func() {
        var wg sync.WaitGroup
        err := filepath.Walk(root, func(path string, info os.FileInfo, err error) error {
            if nil != err {
                return err
            }

            if !info.Mode().IsRegular() {
                return nil
            }

            wg.Add(1)   // raise the semaphore
            go func() {
                data, err := ioutil.Readfile(path)
                select {
                    case c <-result{path, md5, Sum(data), err}:
                    case <-done:
                }

                wg.Done()
            }()

            select {
                case <-done:
                    return errors.New("walk canceled")
                default:
                    return nil
            }
        })
        
        // All calls to wg.Add are done here.
        // Close c when all sends are done.
        go func() {
            wg.Wait()
            close(c)
        }()

        errc <- err // errc is buffered so no select needed.
    }()

    return c, errc
}
#+end_example

`MD5All` receives the digest values from `c` and returns early on error, closing `done`.
#+begin_example
func MD5All(root string) (map[string][md5.Size]byte. error) {
    done := make(chan struct{})
    defer close(done)
    c, errc := sumFiles(done, root)
    m := make(map[string][md5.Size]byte)
    for r := range c {
        if nil != r.err {
            return nil, r.err
        }

        m[r.path] = r.sum
    }

    if err := <-errc; nil != err {
        return nil, err
    }

    return m, nil
}
#+end_example

See that nowhere in `MD5All` anything is being sent to `done`.
Closing the channel is a message in itself.

***** Bounded parallelism
The current `MD5All` implementation starts a new goroutine for each file.
That may cause high memory usage, exceeding available resources.

To limit these allocations, we can bound the number of files being processed in
parallel.
It can by done by creating a fixed number of goroutines for reading files.
The pipeline would then consist of three stages:
****** walk the tree
****** read and digest the files
****** collect the digests

(This looks an awful lot like map/reduce.)

The first stage emits the paths of regular files in the tree:
#+begin_example
func walkFiles(done <-chan struct{}, root string) (<-chan string, <-chan error) {
    paths := make(chan string)
    errc := make(chan error, 1)
    go func () {
        defer close(paths)
        errc <- filepath.Walk(root, func(path string, info os.FileInfo, err error) error {
            if nil != err {
                return err
            }

            if !info.Mode().IsRegular() {
                return nil
            }

            select {
                case paths <-path:
                case <-done:
                    return errors.New("walk canceled")
            }

            return nil
        })
    }()

    return paths, errc
}
#+end_example

The nest stage starts a fixed number of `digester` goroutines, looking as follows.
#+begin_example
func digester(done <-chan struct{}, paths <-chan string, c chan<- result) {
    for path := range paths {
        data, err := ioutil.ReadFile(path)
        select {
            case c <- result{path, md5.Sum(data), err}:
            case <-done:
                return
        }
    }
}
#+end_example

It is important for `digester` not to close its output channel, as multiple goroutines
are sending on it.
It's the job of `MD5All` to arrange for `c` to be closed when all `digester`s are done.
#+begin_example
c := make(chan result)
var wg sync.WaitGroup
const numDigesters = 20
wg.Add(numDigesters)
for i := 0; i < numDigesters; i++ {
    go func() {
        digester(done, paths, c)
        wg.Done()
    }()
}
go func() {
    wg.Wait()
    close(c)
}()
#+end_example

The alternative here would be for each `digester` to create and return its own output
channel - this would require fanning them in within `MD5All`.

The final stage receives all the `result`s and then checks `errc`.
It's important nto to check `errc` any sooner, because `walkFiles` may have not
finished yet.
#+begin_example
m := make(map[string][md5.Size]byte)
for r := range c {
    if nil != r.err {
        return nil, r.err
    }

    m[r.path] = r.sum
}

if err := <-errc: nil != err {
    return nil, err
}

return m, err
#+end_example

My observation is that there is not much sense to returning `nil` in case of an error,
as the failure may not be total.
If we were to return `nil` on any error occurence, it would make sense to break all
computations in that moment.
*** Libraries
**** Go - Libraries
# %toc

***** Logging
****** [[https://github.com/davecgh/go-spew][go-spew]]
****** [[https://github.com/golang/glog][glog]] ported from C++ by Rob Pike
****** [[http://godoc.org/github.com/dgryski/trifles/go-wtflog][wtflog]], a more funny alternative, logging levels might be quite useful though.

***** Profiling
****** perftools' [[http://golang.org/pkg/net/http/pprof/][pprof]]

***** Utility
****** [[https://github.com/tobyhede/go-underscore]], like underscore.js. (under heavy development right now, keep an eye on that one)
*** Profiling
**** Go - Profiling

***** Tip for profiling a single test
To run and profile a single test, it must be run in the context of a benchmark.
Moreover, any other tests must be blocked from running.
To do so, use the following:

#+begin_example
go test -run="^$" -bench="TheTestToProfile" -cpuprofile="out.prof"
#+end_example

`-run="^$"` will bar any tests from being actually run - thus not having their 
stack samples collide with the profiler output.


***** Built-in profiling capabilities
Introductory note - Go's [[htgolangtp://golang.org/pkg/testing/][testing]] package provides a bult-in `benchmark` functionality.
Functions of the form
#+begin_example
func BenchmarkXxxx(*testing.B)
#+end_example

are considered benchmarks and are executed by `go test -bench`.
Benchmarks are run sequentially.

A sample benchmark function might look like so:
#+begin_example
func BenchmarkHello(b *testing.B) {
    for i := 0; i < b.N; i++ {
        fmt.Sprintf("hello")
    }
}
#+end_example

Note that the benchmark must run the code `b.N` times.

Some profiling flags such as `-cpuprofile` and `-memprofile` are available. Check the link for more info.

***** Using the pprof tool

(from http://blog.golang.org/profiling-go-programs)

When not using thet testing builtins, custom profile flags should be defined:

#+begin_example
var cpuprofile = flag.String("cpuprofile", "", "write cpu profile to file")

func main() {
    flag.Parse()
    if *cpuprofile != "" {
        f, err := os.Create(*cpuprofile)    // cpuprofile is a file.
        if err != nil {
            log.Fatal(err)
        }
        pprof.StartCPUProfile(f)
        defer pprof.StopCPUProfile()
    }
#+end_example

Then, we can use the new flag:

#+begin_example
$ make havlak1.prof
./havlak1 -cpuprofile=havlak1.prof
# of loops: 76000 (including 1 artificial root node)
$ go tool pprof havlak1 havlak1.prof
Welcome to pprof!  For help, type 'help'.
(pprof)
#+end_example

`go tool pprof` is a variant of Google's [[https://code.google.com/p/gperftools/wiki/GooglePerformanceTools][`pprof` C++ profiler]].
Important command: `topN`:

#+begin_example
(pprof) top10
Total: 2525 samples
     298  11.8%  11.8%      345  13.7% runtime.mapaccess1_fast64
     268  10.6%  22.4%     2124  84.1% main.FindLoops
     251   9.9%  32.4%      451  17.9% scanblock
     178   7.0%  39.4%      351  13.9% hash_insert
     131   5.2%  44.6%      158   6.3% sweepspan
     119   4.7%  49.3%      350  13.9% main.DFS
      96   3.8%  53.1%       98   3.9% flushptrbuf
      95   3.8%  56.9%       95   3.8% runtime.aeshash64
      95   3.8%  60.6%      101   4.0% runtime.settype_flush
      88   3.5%  64.1%      988  39.1% runtime.mallocgc
#+end_example

A profiled program stops about 100 times / sec and records a sample consisting of the program counters on the currently executing goroutine's stack.
To sort by 4th and 5th columns, use the `-cum` (cumulative) flag.

The percentage might not be 100% even when it theoretically should.
This is due to the fact that each stack sample includes only the bottom 100 frames.

The `web` command draws a graph of the profile data in SVG format and opens it in a web browser. Requires [[http://www.graphviz.org/][graphviz]].

****** Each box in the graph corresponds to a function, and is sized accoring to the number of samples in which it was running.
****** An edge from box X to Y indicates that X calls Y.
****** The number along the edge is the number of times that call appears in a sample.
****** Recursion shows as an edge to self with a number (weight).
****** To show only samples including a specific function, e.g. `mapaccess1`, write `web mapaccess1`.

These commands give us a higher level overview of what's going on in the program.

****** Details

To look closely at a specific function, use `list`:

#+begin_example
(pprof) list DFS
Total: 2525 samples
ROUTINE ====================== main.DFS in /home/rsc/g/benchgraffiti/havlak/havlak1.go
   119    697 Total samples (flat / cumulative)
     3      3  240: func DFS(currentNode *BasicBlock, nodes []*UnionFindNode, number map[*BasicBlock]int, last []int, current int) int {
     1      1  241:     nodes[current].Init(currentNode, current)
     1     37  242:     number[currentNode] = current
     .      .  243:
     1      1  244:     lastid := current
    89     89  245:     for _, target := range currentNode.OutEdges {
     9    152  246:             if number[target] == unvisited {
     7    354  247:                     lastid = DFS(target, nodes, number, last, lastid+1)
     .      .  248:             }
     .      .  249:     }
     7     59  250:     last[number[currentNode]] = lastid
     1      1  251:     return lastid
(pprof)
#+end_example

First 3 columns are:
******* the number of samples taken while running that line, 
******* the number of samples taken while running that line OR in code called from that line,
******* the line number in the file.

There are also supplementary commands:
******* `disasm` shows a disassembly instead of an src listing (can show which instructions are expensive).
******* `weblist` shows the source listing in which clicking a line shows the disasm.

`runtime.mallocgc` means that GC has been caugh in a sample.
To find why GC is running during the execution, use `-memprofile`.

Custom `memprofile` might look like so:
#+begin_example
var memprofile = flag.String("memprofile", "", "write memory profile to this file")
...

    FindHavlakLoops(cfgraph, lsgraph)
    if *memprofile != "" {
        f, err := os.Create(*memprofile)
        if err != nil {
            log.Fatal(err)
        }
        pprof.WriteHeapProfile(f)
        f.Close()
        return
    }
#+end_example

Using `go tool` with the different profile causes it to analyse memory allocations:

#+begin_example
$ go tool pprof havlak3 havlak3.mprof
Adjusting heap profiles for 1-in-524288 sampling rate
Welcome to pprof!  For help, type 'help'.
(pprof) top5
Total: 82.4 MB
    56.3  68.4%  68.4%     56.3  68.4% main.FindLoops
    17.6  21.3%  89.7%     17.6  21.3% main.(*CFG).CreateNode
     8.0   9.7%  99.4%     25.6  31.0% main.NewBasicBlockEdge
     0.5   0.6% 100.0%      0.5   0.6% itab
     0.0   0.0% 100.0%      0.5   0.6% fmt.init
(pprof)
#+end_example

The memory profiler only records information for approximately one block perf half megabyte allocated to reduce overhead.

Functions can be listed all the same through `list`, but this time we will have memory usage instead of stack frames listed.

`go tool pprof --inuse_objects` will report allocations instead  of sizes.

It may be usefule to graph the allocations that are causing GC through `web mallocgc`.
This graph may be unreadable though - most parts of your code will allocate something and so large number of nodes with small sample numbers will interfere visually with the big ones.
To display only the nodes that account for at least 10% of the samples, use `go tool pprof --nodefraction=0.1 havlak4 havlak4.prof`.

To presrve performance, you need to take into account memory management, regardless of the fact of using a GC'ed language.
E.g. if your algorithms need a lot of bookkeeping structures, create a cache of some sort prior too using them, instead of recreating a fresh structure on every iteration.

****** Memory statistics

Can be read with `runtime.ReadMemstats(&m)`.
This struct has tons of members.

Useful ones for looking at the heap:
******* `HeapInuse` - no. of bytes in the heap that are allocated,
******* `HeapIdle` - no. of bytes in the heap waiting to be used,
******* `HeapSys` - no. of bytes obtained from the OS,
******* `HeapReleased` - no. of bytes released to the OS.

*Example* - the garbage making program
#+begin_example
func makeBuffer() []byte {
    return make([]byte, rand.Intn(5000000)+5000000
}

func main() {
    pool := make([][]byte,20)
    makes := 0
    for {
        b := makeBuffer()
        makes += 1
        i := rand.Intn(len(pool))
        pool[i] = b
        time.Sleep(time.Second)
    }
}
#+end_example

How the profile looks like:
{{http://blog.cloudflare.com/static/images/garbage.png}}
`HeapInuse` plateaus at about 150m bytes due to the fixed size of the buffer.
It's visible though that `HeapSys` is about 2.5x more than the program actually needs to have.

This pattern is common in GCed programs - idle memory gets reused and rarely gets released to the OS.

Manual memory mgmt can be used to solve it - using a channel allows to keep a separate pool of unused buffers.
This pool can be then used to retrieve a buffer or make a new one if the channel is empty.

#+begin_example
package main

import (
    "fmt"
    "math/rand"
    "runtime"
    "time"
)

func makeBuffer() []byte {
    return make([]byte, rand.Intn(5000000)+5000000)
}

func main() {
    pool := make([][]byte, 20)

    buffer := make(chan []byte, 5)

    var m runtime.MemStats
    makes := 0
    for {
        var b []byte
        select {
        case b = <-buffer:
        default:
            makes += 1
            b = makeBuffer()
        }

        i := rand.Intn(len(pool))
        if pool[i] != nil {
            select {
            case buffer <- pool[i]:
                pool[i] = nil
            default:
            }
        }

        pool[i] = b

        time.Sleep(time.Second)

        bytes := 0
        for i := 0; i < len(pool); i++ {
            if pool[i] != nil {
                bytes += len(pool[i])
            }
        }

        runtime.ReadMemStats(&m)
        fmt.Printf("%d,%d,%d,%d,%d,%d
", m.HeapSys, bytes, m.HeapAlloc,
            m.HeapIdle, m.HeapReleased, makes)
    }
}
#+end_example

The results:
{{http://blog.cloudflare.com/static/images/garbage-pool.png}}

Now, utilization of memory is nearly 100%.

The key to this memory recycling mechanism is a buffered channel `buffer`.
When the program needs a buffer, it first tries to read one from the channel:
#+begin_example
select {
    case b <- buffer:
    default:
        b := makeBuffer()
}
#+end_example

This either places a retrieved slice in the buffer or creates a new one, if the channel is empty.

To put slices back into the channel, do a similar thing:
#+begin_example
select {
    case buffer <- pool[i]:
        pool[i] = nil
    default:
}
#+end_example

If the `buffer` channel is full, then nothing is being done to avoid blocking.
Note that this pool can be even reused across goroutines due to the nature of channels.

Cloudflare buffer recycler works by having a goroutine that handles creation of buffers and sharing them across other goroutines.
Two channels: `get` (to get a new buffer) and `give` (to return a buffer to the pool) are used for all communication.
Internally, the recycler keeps a linked list of returned buffers and periodically removes the ones that are too old and unlikely to be reused.
That allows to cope with bursts of demand.
#+begin_example
package main

import (
    "container/list"
    "fmt"
    "math/rand"
    "runtime"
    "time"
)

var makes int
var frees int

func makeBuffer() []byte {
    makes += 1
    return make([]byte, rand.Intn(5000000)+5000000)
}

type queued struct {
    when time.Time
    slice []byte
}

func makeRecycler() (get, give chan []byte) {
    get = make(chan []byte)
    give = make(chan []byte)

    go func() {
        q := new(list.List)
        for {
            if q.Len() == 0 {
                q.PushFront(queued{when: time.Now(), slice: makeBuffer()})
            }

            e := q.Front()

            timeout := time.NewTimer(time.Minute)
            select {
            case b := <-give:
                timeout.Stop()
                q.PushFront(queued{when: time.Now(), slice: b})

           case get <- e.Value.(queued).slice:
               timeout.Stop()
               q.Remove(e)

           case <-timeout.C:
               e := q.Front()
               for e != nil {
                   n := e.Next()
                   if time.Since(e.Value.(queued).when) > time.Minute {
                       q.Remove(e)
                       e.Value = nil
                   }
                   e = n
               }
           }
       }

    }()

    return
}

func main() {
    pool := make([][]byte, 20)

    get, give := makeRecycler()

    var m runtime.MemStats
    for {
        b := <-get
        i := rand.Intn(len(pool))
        if pool[i] != nil {
            give <- pool[i]
        }

        pool[i] = b

        time.Sleep(time.Second)

        bytes := 0
        for i := 0; i < len(pool); i++ {
            if pool[i] != nil {
                bytes += len(pool[i])
            }
        }

        runtime.ReadMemStats(&m)
        fmt.Printf("%d,%d,%d,%d,%d,%d,%d
", m.HeapSys, bytes, m.HeapAlloc
             m.HeapIdle, m.HeapReleased, makes, frees)
    }
}
#+end_example

Running that looks very similar to the second version.
{{http://blog.cloudflare.com/static/images/garbage-recyler.png}}

Any arbitrary type can be reused in that manner, not only `[]byte` slices.

****** Benchmark visualization

https://github.com/ajstarks/svgo/blob/master/benchviz/benchviz.go can be used to visualize benchmark results.
*** GoPractices
**** Go - Practices
# %toc

***** Use a single $GOPATH

If your project is very big and important, it deserves a separate `$GOPATH`.
Until that time though, don't try to use multiple `$GOPATH`s.
It will just slow you down.

***** Wrap for-select idiom in a function

`for-select` is the bread and butter of concurrent programming in Go.

If there is a situation where you need to break out of such loop, labels have to be used.
Instead, wrap the loop in a function - it will be much easier to just `return` from it.
Also, you gain the opportunity to return an error.

Example comparison:
#+begin_example
// The wrong way
func main() {

L:
    for {
        select {
        case <-time.After(time.Second):
            fmt.Println("hello")
        default:
            break L
        }
    }

    fmt.Println("ending")
}

// The right way
func main() {
    foo()
    fmt.Println("ending")
}

func foo() {
    for {
        select {
        case <-time.After(time.Second):
            fmt.Println("hello")
        default:
            return
        }
    }
}
#+end_example

***** Always use tagged literals

What it means - use named properties while instantiating structs ad-hoc.
Example:
#+begin_example
type T struct {
    Foo string
    Bar int
    Qux string
}

func main() {
    t := T{Foo: "example", Bar: 123}
    fmt.Printf("t %+v
", t)
}
#+end_example

Otherwise, the compiler will throw an error when the struct changes.
On the other hand, it might be intentional.

***** Split initializations into multiple lines

Well, duh.
It scales.
Remember after the comma after each line, including the last one.

***** Add String() method for integers const values

It increases readability - it's an equivalent of overriding `toString()` in Java or C#.

#+begin_example
type State int

const (
    Running State = iota 
    Stopped
    Rebooting
    Terminated
)

func (s State) String() string {
    switch s {
    case Running:
        return "Running"
    case Stopped:
        return "Stopped"
    case Rebooting:
        return "Rebooting"
    case Terminated:
        return "Terminated"
    default:
        return "Unknown"
    }
}

func main() {
    state := Running

    // print: "state 0"
    fmt.Println("state ", state)
}
#+end_example

***** Start iota with a +1 increment

This tip is bullshit.
`iota+1` seems to be the equivalent of `undefined` in JS.
It returns `Unknwown` when `String()`ed.

A better alternative (related to the previous example) would be to do the following.
#+begin_example
const (
    Unknown State = iota 
    Running
    Stopped
    Rebooting
    Terminated
)
#+end_example

***** Return function calls

Instead of
#+begin_example
func bar() (string, error) {
    v, err := foo()
    if err != nil {
        return "", err
    }

    return v, nil
}
#+end_example

Return this.
#+begin_example
func bar() (string, error) {
    return foo()
}
#+end_example

***** Convert slices, maps etc. into custom types

Instead of a `map[string][string]`, use a clear type name for the sake of readability.
The second benefit here is extensibility, as you can now add methods to instances of that type.

***** Use contextual wrapper functions

Abstract error handling, locking, DB connections etc. into contextual wrappers.
#+begin_example
func withLockContext(fn func()) {
    mu.Lock
    defer mu.Unlock()

    fn()
}

func foo() {
    withLockContext(func() {
        // foo related stuff
    })
}

func withDBContext(fn func(db DB)) error {
    // get a db connection from the connection pool
    dbConn := NewDB()

    return fn(dbConn)
}

func foo() {
    withDBContext(func(db *DB) error {
        // foo related stuff
    })
}
#+end_example

***** Add setter,getters for map access

A prime example is of concurrent access to a map.
What if one goroutine does `m["foo"] = bar` and another one `delete(m, "foo")`?

Encapsulate.
#+begin_example
func Put(key, value string) {
    mu.Lock()
    m[key] = value
    mu.Unlock()
}
func Delete(key string) {
    mu.Lock()
    delete(m, key)
    mu.Unlock()
}
#+end_example

One step further - use an interface to abstract storage away.

#+begin_example
type Storage interface {
    Delete(key string)
    Get(key string) string
    Put(key, value string)
}
#+end_example

*Note:*
Sometimes, interfaces are overkill.
You might need to lock several variables at once.
This would lead to layering of interaces.
Apply this improvement only if it does not bring too much additional complexity to the table.
*** Resources

**** [[https://gobyexample.com/][Go by example]] - a massive list of useful code examples, presenting how specific aspects of Go work.
**** [[http://go-lang.cat-v.org/go-code][List of useful Go apps and packages]]

*** Quirks and issues

**** Using the `html/template` package

A problem with templates not rendering:

When I run:
#+begin_example
t, _ := template.ParseFiles("index.html")
t.Execute(w, nil)
#+end_example
the page loads fine. But when I try and run
#+begin_example
t := template.New("first")
t, _ = t.ParseFiles("index.html")
t.Execute(w, nil)
#+end_example
the only thing that loads is a blank page.

The solution:

    The first version works as you expect because the package-level `ParseFiles`
    function will return a new template that has the name and content of the
    first parsed file. 
    In the second case, though, you're creating a template named `"first"` and
    then parsing one with name `"index.html"`.
    When you call `t.Execute` on `"first"`, it's still empty.
    You can fix the problem by either: Using `template.New("index.html")`, so that
    the file name matches the template name you parse next or Providing the
    template name you want to execute explicitly with
    `t.ExecuteTemplate(w, "index.html", nil)`.


**** Using io.Reader in an efficient manner

(from https://www.datadoghq.com/2014/07/crossing-streams-love-letter-gos-io-reader/)

Avoid using `readAll` and similar methods - make use of streams (work with `read` instead).
Remember how you program in UNIX - piping output etc. There are no intermediate state files storing the data e.g.:

#+begin_example
# this is how we program for some reason
ls > files.txt
grep "foo" files.txt > grepped.txt
wc -l grepped.txt
rm files.txt grepped.txt
#+end_example

Programming with intermediate state files is a more imperative approach, and such thinking almost always leads to inefficiencies.

Example of bad vs good:

BAD
#+begin_example
func LoadGzippedJSON(r io.Reader, v interface{}) error {
    data, err := ioutil.ReadAll(r)
    if err != nil {
        return err
    }
    // oh wait, we need a Reader again.. 
    raw := bytes.NewBuffer(data)
    unz, err := gzip.NewReader(raw)
    if err != nil {
        return err
    }
    buf, err := ioutil.ReadAll(unz)
    if err != nil {
        return err
    }
    return json.Unmarshal(buf, &v)
}
#+end_example

GOOD
#+begin_example
func LoadGzippedJSON(r io.Reader, v interface{}) error {
    raw, err := gzip.NewReader(r)
    if err != nil {
        return err
    }
    return json.NewDecoder(raw).Decode(&v)
}
#+end_example
