#+FILETAGS: :vimwiki:

* 2015-01-17
** Chamberconf 1

*** To nie zawsze wina aplikacji

Bandwidth testing - use different packet sizes, traffic shapes etc.

Kirk Pepperdine performance materials.
Weave monitoring into the application during the deisgn phase - account for possible numbers(resource usage, data flow).
Charting these values may provide new perspective.

JavaMelody, Zabbix, Nagios, Ganglia

slideshare d0cent high-availability introduction

A backup copy has to go to a geographically separated location to avoid natural disasters.

Recovery Point Objective - how much data is lost when restoring
Recovery Time Objective - how much time it takes to restore

snapshot/replica is not a proper backup:
**** it is not geographically separated
**** snapshots are not permanent - they're being overwritten, sometimes without our consent

**** Network topology

Switchers prefer big packets - the capacities highlghted by vendors are achievable in favorable conditions.
Dense small packet traffic can cause jams nevertheless.
Przepływność na ang. - symbol rate?

Defense in depth

**** Sharing the infrasttructure

Net, disks, fs, virtualization can be shared between products, internal systems, presales, preproduction.

Remember about system load when creating a lot of virtual machines on a single host - the system has to do context switching.
It compounds with the user load, giving a higher total load than reported by default metrics.

vMotion can cause resource transfe loops.

Xen and LXC mentioned as VM alternatives.
Light virtualizations do not pretend that we have more resources than we actually do.

Containers allow lbrary sharing.

If the infrastructure sucks, show load tests to prove it.

Create SLAs for when will your application be working.

Performance lab?

There is no one-size-fits-all solution, but Nagios is a bit too complicated for starters - there is a lot of data nobody knows what to do with.
Simple tools - iostat on the server. For the first few days don't aggregate the data, just try to understand what is going on.
Measure the key characteristics that are of utmost importance for your application.

**** Reactive impl w/Akka

**** Distributed Consesus aka what do we eat for lunch

Consensus = Termination + Validity + Integrity + Agreement
Distributed system - a system where pariticpants communicate asynchronously using messages.

Failure detection in such a setting must rely on external knowledge, most likely supported by time.
Byzantine generals problem...
Actor description...
Fallacies of distributed computing...

Failure modes describe how a component might fail.
***** fail-stop
***** fail-recover: recovering might cause data inconsistencies (as in nonsensical data)
***** byzantine: a node can fail, can respond with incorrect values and intends to be malicious - the data can look ok, but is targeted to harm the system. This requires extreme amounts of messaging to consent on something.

Use failstop or failrecover for 99% normal cases.

***** 2-phase commit
****** Propose value to all instances
****** Commit the write after getting acknowledged.

Problems:
****** propose value and die
****** propose value to 1 node and die

Proposing needs timeouts (uncommited propositions must be flushed).

Timeouts + recovery commiter - the commiter asks the nodes what's been said (propose or commit).
If at least one node sees a commited state, then the recovery commiter can send the commit to the rest.
If the commiter dies, we're fucked.

***** Quorum
First node has a value and tells other to vote for it.
If a node does not have anything to vote on, it votes on the first value received.
If no majority can be reached, an additional round of voting has to be performed.
This is cool, but cannot be guaranteed to terminate - there can always be infinite ties.

***** Paxos

Basic paxos - choose exactly one value. 
Aside: JavaZone paxos presentation?
oCore idea is based around a replicated state machine.
If all nodes see the same event history, then they will agree on a single value having this history as its only knowledge source.
Roles: 
Acceptor: 
Proposer, 
Learner, 
Leader: a special kind of Proposer, gets changed on every voting turn.

Proposals are numbered with seq numbers, which e.g. [serverId|roundNr]
2 Phases: Prepare and Accept

Leader increments his sequence number and sends his value along with it to all acceptors.
Every acceptor checks whether he has accepted a value with ahigher sequence number yet.
If not, it responds to the proposer with an acceptance message.
The accepter has to respond with an acceptance message containing the accepted value and its seq num, even if it's not the one the current proposer issued.
If the Proposer received back a different value, it re-sends it to all of the acceptors.
After that, the acceptors send a Learn message to the Learners with the accepted value.

If acceptors accepted a value with a higher sequence number, then the algorithm has to be restarted for this proposer with the higher seq num.

Additions: stable leader, performance *bcast roundtrip trimming), ensure full replication, resiliency to number of actors differing.
This is dealt with in Multi Paxos.

***** Raft
leader based, less processes than paxos, simplicity as a goal

includes snapshotting and membership

Follower - Candidate - Leader

Voting works similarly to quorum.

Candidate votes for himself and requests votes from Followers.
Aside: Try implementing Raft. (Akka FSM DSL is cool)
FLP Impossibility result paper... -> consensus is doomed to fail.
